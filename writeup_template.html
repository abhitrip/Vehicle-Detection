<!DOCTYPE html>
<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8"><meta charset="utf-8"><style>@font-face {
  font-family: octicons-anchor;
  src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}

* {
    box-sizing: border-box;
}

body {
    width: 980px;
    margin-right: auto;
    margin-left: auto;
}

body .markdown-body {
    padding: 45px;
    border: 1px solid #ddd;
    border-radius: 3px;
    word-wrap: break-word;
}

pre {
    font: 12px Consolas, "Liberation Mono", Menlo, Courier, monospace;
}

.markdown-body {
  -webkit-text-size-adjust: 100%;
  text-size-adjust: 100%;
  color: #333;
  font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
  font-size: 16px;
  line-height: 1.6;
  word-wrap: break-word;
}

.markdown-body a {
  background-color: transparent;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline: 0;
}

.markdown-body strong {
  font-weight: bold;
}

.markdown-body h1 {
  font-size: 2em;
  margin: 0.67em 0;
}

.markdown-body img {
  border: 0;
}

.markdown-body hr {
  box-sizing: content-box;
  height: 0;
}

.markdown-body pre {
  overflow: auto;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre {
  font-family: monospace, monospace;
  font-size: 1em;
}

.markdown-body input {
  color: inherit;
  font: inherit;
  margin: 0;
}

.markdown-body html input[disabled] {
  cursor: default;
}

.markdown-body input {
  line-height: normal;
}

.markdown-body input[type="checkbox"] {
  box-sizing: border-box;
  padding: 0;
}

.markdown-body table {
  border-collapse: collapse;
  border-spacing: 0;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body input {
  font: 13px / 1.4 Helvetica, arial, nimbussansl, liberationsans, freesans, clean, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
}

.markdown-body a {
  color: #4078c0;
  text-decoration: none;
}

.markdown-body a:hover,
.markdown-body a:active {
  text-decoration: underline;
}

.markdown-body hr {
  height: 0;
  margin: 15px 0;
  overflow: hidden;
  background: transparent;
  border: 0;
  border-bottom: 1px solid #ddd;
}

.markdown-body hr:before {
  display: table;
  content: "";
}

.markdown-body hr:after {
  display: table;
  clear: both;
  content: "";
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 15px;
  margin-bottom: 15px;
  line-height: 1.1;
}

.markdown-body h1 {
  font-size: 30px;
}

.markdown-body h2 {
  font-size: 21px;
}

.markdown-body h3 {
  font-size: 16px;
}

.markdown-body h4 {
  font-size: 14px;
}

.markdown-body h5 {
  font-size: 12px;
}

.markdown-body h6 {
  font-size: 11px;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ul,
.markdown-body ol {
  padding: 0;
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ul ul ol,
.markdown-body ul ol ol,
.markdown-body ol ul ol,
.markdown-body ol ol ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code {
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  font-size: 12px;
}

.markdown-body pre {
  margin-top: 0;
  margin-bottom: 0;
  font: 12px Consolas, "Liberation Mono", Menlo, Courier, monospace;
}

.markdown-body .select::-ms-expand {
  opacity: 0;
}

.markdown-body .octicon {
  font: normal normal normal 16px/1 octicons-anchor;
  display: inline-block;
  text-decoration: none;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.markdown-body .octicon-link:before {
  content: '\f05c';
}

.markdown-body:before {
  display: table;
  content: "";
}

.markdown-body:after {
  display: table;
  clear: both;
  content: "";
}

.markdown-body>*:first-child {
  margin-top: 0 !important;
}

.markdown-body>*:last-child {
  margin-bottom: 0 !important;
}

.markdown-body a:not([href]) {
  color: inherit;
  text-decoration: none;
}

.markdown-body .anchor {
  display: inline-block;
  padding-right: 2px;
  margin-left: -18px;
}

.markdown-body .anchor:focus {
  outline: none;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 1em;
  margin-bottom: 16px;
  font-weight: bold;
  line-height: 1.4;
}

.markdown-body h1 .octicon-link,
.markdown-body h2 .octicon-link,
.markdown-body h3 .octicon-link,
.markdown-body h4 .octicon-link,
.markdown-body h5 .octicon-link,
.markdown-body h6 .octicon-link {
  color: #000;
  vertical-align: middle;
  visibility: hidden;
}

.markdown-body h1:hover .anchor,
.markdown-body h2:hover .anchor,
.markdown-body h3:hover .anchor,
.markdown-body h4:hover .anchor,
.markdown-body h5:hover .anchor,
.markdown-body h6:hover .anchor {
  text-decoration: none;
}

.markdown-body h1:hover .anchor .octicon-link,
.markdown-body h2:hover .anchor .octicon-link,
.markdown-body h3:hover .anchor .octicon-link,
.markdown-body h4:hover .anchor .octicon-link,
.markdown-body h5:hover .anchor .octicon-link,
.markdown-body h6:hover .anchor .octicon-link {
  visibility: visible;
}

.markdown-body h1 {
  padding-bottom: 0.3em;
  font-size: 2.25em;
  line-height: 1.2;
  border-bottom: 1px solid #eee;
}

.markdown-body h1 .anchor {
  line-height: 1;
}

.markdown-body h2 {
  padding-bottom: 0.3em;
  font-size: 1.75em;
  line-height: 1.225;
  border-bottom: 1px solid #eee;
}

.markdown-body h2 .anchor {
  line-height: 1;
}

.markdown-body h3 {
  font-size: 1.5em;
  line-height: 1.43;
}

.markdown-body h3 .anchor {
  line-height: 1.2;
}

.markdown-body h4 {
  font-size: 1.25em;
}

.markdown-body h4 .anchor {
  line-height: 1.2;
}

.markdown-body h5 {
  font-size: 1em;
}

.markdown-body h5 .anchor {
  line-height: 1.1;
}

.markdown-body h6 {
  font-size: 1em;
  color: #777;
}

.markdown-body h6 .anchor {
  line-height: 1.1;
}

.markdown-body p,
.markdown-body blockquote,
.markdown-body ul,
.markdown-body ol,
.markdown-body dl,
.markdown-body table,
.markdown-body pre {
  margin-top: 0;
  margin-bottom: 16px;
}

.markdown-body hr {
  height: 4px;
  padding: 0;
  margin: 16px 0;
  background-color: #e7e7e7;
  border: 0 none;
}

.markdown-body ul,
.markdown-body ol {
  padding-left: 2em;
}

.markdown-body ul ul,
.markdown-body ul ol,
.markdown-body ol ol,
.markdown-body ol ul {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body li>p {
  margin-top: 16px;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: bold;
}

.markdown-body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}

.markdown-body blockquote {
  padding: 0 15px;
  color: #777;
  border-left: 4px solid #ddd;
}

.markdown-body blockquote>:first-child {
  margin-top: 0;
}

.markdown-body blockquote>:last-child {
  margin-bottom: 0;
}

.markdown-body table {
  display: block;
  width: 100%;
  overflow: auto;
  word-break: normal;
  word-break: keep-all;
}

.markdown-body table th {
  font-weight: bold;
}

.markdown-body table th,
.markdown-body table td {
  padding: 6px 13px;
  border: 1px solid #ddd;
}

.markdown-body table tr {
  background-color: #fff;
  border-top: 1px solid #ccc;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

.markdown-body img {
  max-width: 100%;
  box-sizing: content-box;
  background-color: #fff;
}

.markdown-body code {
  padding: 0;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
  margin: 0;
  font-size: 85%;
  background-color: rgba(0,0,0,0.04);
  border-radius: 3px;
}

.markdown-body code:before,
.markdown-body code:after {
  letter-spacing: -0.2em;
  content: "\00a0";
}

.markdown-body pre>code {
  padding: 0;
  margin: 0;
  font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}

.markdown-body .highlight {
  margin-bottom: 16px;
}

.markdown-body .highlight pre,
.markdown-body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
  background-color: #f7f7f7;
  border-radius: 3px;
}

.markdown-body .highlight pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre code {
  display: inline;
  max-width: initial;
  padding: 0;
  margin: 0;
  overflow: initial;
  line-height: inherit;
  word-wrap: normal;
  background-color: transparent;
  border: 0;
}

.markdown-body pre code:before,
.markdown-body pre code:after {
  content: normal;
}

.markdown-body kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb;
}

.markdown-body .pl-c {
  color: #969896;
}

.markdown-body .pl-c1,
.markdown-body .pl-s .pl-v {
  color: #0086b3;
}

.markdown-body .pl-e,
.markdown-body .pl-en {
  color: #795da3;
}

.markdown-body .pl-s .pl-s1,
.markdown-body .pl-smi {
  color: #333;
}

.markdown-body .pl-ent {
  color: #63a35c;
}

.markdown-body .pl-k {
  color: #a71d5d;
}

.markdown-body .pl-pds,
.markdown-body .pl-s,
.markdown-body .pl-s .pl-pse .pl-s1,
.markdown-body .pl-sr,
.markdown-body .pl-sr .pl-cce,
.markdown-body .pl-sr .pl-sra,
.markdown-body .pl-sr .pl-sre {
  color: #183691;
}

.markdown-body .pl-v {
  color: #ed6a43;
}

.markdown-body .pl-id {
  color: #b52a1d;
}

.markdown-body .pl-ii {
  background-color: #b52a1d;
  color: #f8f8f8;
}

.markdown-body .pl-sr .pl-cce {
  color: #63a35c;
  font-weight: bold;
}

.markdown-body .pl-ml {
  color: #693a17;
}

.markdown-body .pl-mh,
.markdown-body .pl-mh .pl-en,
.markdown-body .pl-ms {
  color: #1d3e81;
  font-weight: bold;
}

.markdown-body .pl-mq {
  color: #008080;
}

.markdown-body .pl-mi {
  color: #333;
  font-style: italic;
}

.markdown-body .pl-mb {
  color: #333;
  font-weight: bold;
}

.markdown-body .pl-md {
  background-color: #ffecec;
  color: #bd2c00;
}

.markdown-body .pl-mi1 {
  background-color: #eaffea;
  color: #55a532;
}

.markdown-body .pl-mdr {
  color: #795da3;
  font-weight: bold;
}

.markdown-body .pl-mo {
  color: #1d3e81;
}

.markdown-body kbd {
  display: inline-block;
  padding: 3px 5px;
  font: 11px Consolas, "Liberation Mono", Menlo, Courier, monospace;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb;
}

.markdown-body .plan-price-unit {
  color: #767676;
  font-weight: normal;
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  margin: 0 0.35em 0.25em -1.6em;
  vertical-align: middle;
}

.markdown-body .plan-choice {
  padding: 15px;
  padding-left: 40px;
  display: block;
  border: 1px solid #e0e0e0;
  position: relative;
  font-weight: normal;
  background-color: #fafafa;
}

.markdown-body .plan-choice.open {
  background-color: #fff;
}

.markdown-body .plan-choice.open .plan-choice-seat-breakdown {
  display: block;
}

.markdown-body .plan-choice-free {
  border-radius: 3px 3px 0 0;
}

.markdown-body .plan-choice-paid {
  border-radius: 0 0 3px 3px;
  border-top: 0;
  margin-bottom: 20px;
}

.markdown-body .plan-choice-radio {
  position: absolute;
  left: 15px;
  top: 18px;
}

.markdown-body .plan-choice-exp {
  color: #999;
  font-size: 12px;
  margin-top: 5px;
}

.markdown-body .plan-choice-seat-breakdown {
  margin-top: 10px;
  display: none;
}

.markdown-body :checked+.radio-label {
  z-index: 1;
  position: relative;
  border-color: #4078c0;
}
</style><title>README</title></head><body><article class="markdown-body"><hr>
<p><strong>Vehicle Detection Project</strong></p>
<p>The goals / steps of this project are the following:</p>
<ul>
<li>Perform a Histogram of Oriented Gradients (HOG) feature extraction 
on a labeled training set of images and train a classifier Linear SVM 
classifier</li>
<li>Optionally, you can also apply a color transform and append binned 
color features, as well as histograms of color, to your HOG feature 
vector.</li>
<li>Note: for those first two steps don't forget to normalize your features and randomize a selection for training and testing.</li>
<li>Implement a sliding-window technique and use your trained classifier to search for vehicles in images.</li>
<li>Run your pipeline on a video stream (start with the test_video.mp4 
and later implement on full project_video.mp4) and create a heat map of 
recurring detections frame by frame to reject outliers and follow 
detected vehicles.</li>
<li>Estimate a bounding box for vehicles detected.</li>
</ul>
<h2>
<a id="user-content-rubric-points" class="anchor" href="#rubric-points" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://review.udacity.com/#%21/rubrics/513/view">Rubric</a> Points</h2>
<h3>
<a id="user-content-here-i-will-consider-the-rubric-points-individually-and-describe-how-i-addressed-each-point-in-my-implementation" class="anchor" href="#here-i-will-consider-the-rubric-points-individually-and-describe-how-i-addressed-each-point-in-my-implementation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Here I will consider the rubric points individually and describe how I addressed each point in my implementation.</h3>
<hr>
<h3>
<a id="user-content-writeup--readme" class="anchor" href="#writeup--readme" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Writeup / README</h3>
<h4>
<a id="user-content-1-provide-a-writeup--readme-that-includes-all-the-rubric-points-and-how-you-addressed-each-one--you-can-submit-your-writeup-as-markdown-or-pdf--here-is-a-template-writeup-for-this-project-you-can-use-as-a-guide-and-a-starting-point" class="anchor" href="#1-provide-a-writeup--readme-that-includes-all-the-rubric-points-and-how-you-addressed-each-one--you-can-submit-your-writeup-as-markdown-or-pdf--here-is-a-template-writeup-for-this-project-you-can-use-as-a-guide-and-a-starting-point" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1.
 Provide a Writeup / README that includes all the rubric points and how 
you addressed each one.  You can submit your writeup as markdown or pdf.
  <a href="https://github.com/udacity/CarND-Vehicle-Detection/blob/master/writeup_template.md">Here</a> is a template writeup for this project you can use as a guide and a starting point.</h4>
<p>You're reading it! The code and plots can be seen in ./vehicle-detection.ipynb.</p>
<h3>
<a id="user-content-histogram-of-oriented-gradients-hog" class="anchor" href="#histogram-of-oriented-gradients-hog" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Histogram of Oriented Gradients (HOG)</h3>
<h4>
<a id="user-content-1-explain-how-and-identify-where-in-your-code-you-extracted-hog-features-from-the-training-images" class="anchor" href="#1-explain-how-and-identify-where-in-your-code-you-extracted-hog-features-from-the-training-images" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1. Explain how (and identify where in your code) you extracted HOG features from the training images.</h4>
<p>I started by reading in all the <code>vehicle</code> and <code>non-vehicle</code> images.  The code is there in "Load data cells".Here are some examples from each of the <code>vehicle</code> and <code>non-vehicle</code> classes:</p>
<p><a href="file:///Users/atripathy/Vehicle-Detection/fig1.jpg" target="_blank"><img src="writeup_files/fig1.jpg" alt="alt text" style="max-width:100%;"></a>
I then explored different color spaces and different <code>skimage.hog()</code> parameters (<code>orientations</code>, <code>pixels_per_cell</code>, and <code>cells_per_block</code>).  I grabbed random images from each of the two classes and displayed them to get a feel for what the <code>skimage.hog()</code> output looks like.
Here is an example using the <code>YUV</code> color space and HOG parameters of <code>orientations=8</code>, <code>pixels_per_cell=(8, 8)</code> and <code>cells_per_block=(2, 2)</code>. The figure shows the comparison of hog features between a car image and a non-car one. The code is there in <code>get_hog_features()</code> :</p>
<p><a href="file:///Users/atripathy/Vehicle-Detection/fig2.jpg" target="_blank"><img src="writeup_files/fig2.jpg" alt="alt text" style="max-width:100%;"></a>
Finally to compute the features of all the samples, I use the <code>extract_features()</code> in the <strong>Extract Hog features</strong>
 cell. It accepts a list of image paths and HOG parameters and produces a
 flattened array of HOG features for each image in the list.</p>
<p>####2. Explain how you settled on your final choice of HOG parameters.</p>
<p>I tried various combinations of colorspaces X orientations X Pixels 
per cell X Cells per block X HOG channels and settled on the final 
choice based on the performance of SVC classifier produced using them.<br>
The final parameters I used are documented below:</p>
<ul>
<li>colorspace = 'YUV'</li>
<li>orient = 11</li>
<li>pix_per_cell = 16</li>
<li>cell_per_block = 2</li>
<li>hog_channel = 'ALL'</li>
</ul>
<p>####3. Describe how (and identify where in your code) you trained a 
classifier using your selected HOG features (and color features if you 
used them).</p>
<p>I have used only HOG features and didn't use either spatial intensity
 or channel intensity features. In the section titled "Train Classifier"
 I trained a linear SVM with the default classifier parameters. The 
linear SVM Classifier was able to achieve a test accuracy of 97.94%.</p>
<p>###Sliding Window Search</p>
<h4>
<a id="user-content-1describe-how-and-identify-where-in-your-code-you-implemented-a-sliding-window-search--how-did-you-decide-what-scales-to-search-and-how-much-to-overlap-windows" class="anchor" href="#1describe-how-and-identify-where-in-your-code-you-implemented-a-sliding-window-search--how-did-you-decide-what-scales-to-search-and-how-much-to-overlap-windows" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1.Describe
 how (and identify where in your code) you implemented a sliding window 
search.  How did you decide what scales to search and how much to 
overlap windows?</h4>
<p>I used the method <code>find_cars()</code> from the lecture content. 
The method combines HOG feature extraction with a sliding window search,
 but rather than perform feature extraction on each window individually 
which can be time consuming, the HOG features are extracted for the 
entire image (or a selected portion of it) just once. Then these 
full-image features are subsampled according to the size of the window 
and then fed to the classifier. The method performs the classifier 
prediction on the HOG features for each window region and returns a list
 of rectangle objects corresponding to the windows that generated a 
positive ("car") prediction.</p>
<p>To decide scale, window start and stop positions, I experimented with
 various combinations of parameters. I found out that small scales ~ 0.5
 returned too many positives. So to reduce them, I used scales of 
1,1.5,2 and 3.5. You can see them in the <code>Combine Sliding window search</code> cell of the Notebook. They contain the values. Below, You can see the rectangles returned by <code>find_cars()</code>
on one of the test images.</p>
<p><a href="file:///Users/atripathy/Vehicle-Detection/fig3.jpg" target="_blank"><img src="writeup_files/fig3.jpg" alt="alt text" style="max-width:100%;"></a>
Now, we know that a true positive is typically accompanied by several 
positive detections, while false positives are typically accompanied by 
only one or two detections. We employ a combined heatmap and threshold  
to differentiate the two. The add_heat function increments the pixel 
value (referred to as "heat") of an all-black image the size of the 
original image at the location of each detection rectangle. Areas 
encompassed by more overlapping rectangles are assigned higher levels of
 heat. To illustrate the above, first see the image heatmap with a false
 positive:</p>
<p><a href="file:///Users/atripathy/Vehicle-Detection/fig5.jpg" target="_blank"><img src="writeup_files/fig5.jpg" alt="alt text" style="max-width:100%;"></a>
Now, after applying threshold, we remove the false positve. You can see the thresholded heatmap as :</p>
<p><a href="file:///Users/atripathy/Vehicle-Detection/fig6.jpg" target="_blank"><img src="writeup_files/fig6.jpg" alt="alt text" style="max-width:100%;"></a>.
The scipy.ndimage.measurements.label() function collects spatially contiguous areas of the heatmap and assigns each a label:</p>
<p><a href="file:///Users/atripathy/Vehicle-Detection/fig7.jpg" target="_blank"><img src="writeup_files/fig7.jpg" alt="alt text" style="max-width:100%;"></a>.
Now finally, we take the minimum and maximum values of x and y 
co-ordinates to compute the final windows inside which more or less our 
cars lie. We get a result as:</p>
<p><a href="file:///Users/atripathy/Vehicle-Detection/fig8.jpg" target="_blank"><img src="writeup_files/fig8.jpg" alt="alt text" style="max-width:100%;"></a></p>
<h4>
<a id="user-content-2show-some-examples-of-test-images-to-demonstrate-how-your-pipeline-is-working--what-did-you-do-to-optimize-the-performance-of-your-classifier" class="anchor" href="#2show-some-examples-of-test-images-to-demonstrate-how-your-pipeline-is-working--what-did-you-do-to-optimize-the-performance-of-your-classifier" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2.Show
 some examples of test images to demonstrate how your pipeline is 
working.  What did you do to optimize the performance of your 
classifier?</h4>
<p>The final implementation performs reasonably well with no false 
positives in the test images. Initially I chose only The Y channel of 
YUV image. It's accuracy never topped 97%, so I used all three channels 
of YUV image. That improved the accuracy to 98.51%. Also, increasing the
 pixels_per_cell from 8 to 16 greatly reduced the execution time. Other 
optimization techniques included changes to window sizing and overlap as
 described above, and lowering the heatmap threshold to improve accuracy
 of the detection.
The performance on <code>test images</code> are shown below: <a href="file:///Users/atripathy/Vehicle-Detection/fig9.jpg" target="_blank"><img src="writeup_files/fig9.jpg" alt="alt text" style="max-width:100%;"></a></p>
<hr>
<h3>
<a id="user-content-video-implementation" class="anchor" href="#video-implementation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Video Implementation</h3>
<h4>
<a id="user-content-1provide-a-link-to-your-final-video-output--your-pipeline-should-perform-reasonably-well-on-the-entire-project-video-somewhat-wobbly-or-unstable-bounding-boxes-are-ok-as-long-as-you-are-identifying-the-vehicles-most-of-the-time-with-minimal-false-positives" class="anchor" href="#1provide-a-link-to-your-final-video-output--your-pipeline-should-perform-reasonably-well-on-the-entire-project-video-somewhat-wobbly-or-unstable-bounding-boxes-are-ok-as-long-as-you-are-identifying-the-vehicles-most-of-the-time-with-minimal-false-positives" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1.Provide
 a link to your final video output.  Your pipeline should perform 
reasonably well on the entire project video (somewhat wobbly or unstable
 bounding boxes are ok as long as you are identifying the vehicles most 
of the time with minimal false positives.)</h4>
<p>Here's a <a href="file:///Users/atripathy/Vehicle-Detection/project_video.mp4">link to my video result</a>.You can see it here too:</p>
<h4>
<a id="user-content-2-describe-how-and-identify-where-in-your-code-you-implemented-some-kind-of-filter-for-false-positives-and-some-method-for-combining-overlapping-bounding-boxes" class="anchor" href="#2-describe-how-and-identify-where-in-your-code-you-implemented-some-kind-of-filter-for-false-positives-and-some-method-for-combining-overlapping-bounding-boxes" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2.
 Describe how (and identify where in your code) you implemented some 
kind of filter for false positives and some method for combining 
overlapping bounding boxes.</h4>
<p>The code for processing frames of video is contained in the cell 
titled "Pipeline for Processing Video Frames" and is identical to the 
code for processing a single image described above, with the exception 
of storing the detections (returned by find_cars) from the previous 15 
frames of video using the prev_rects parameter from a class called 
Vehicle_Detect. Rather than performing the heatmap/threshold/label steps
 for the current frame's detections, the detections for the past 15 
frames are combined and added to the heatmap and the threshold for the 
heatmap is set to 1 + len(det.prev_rects)//2 (one more than half the 
number of rectangle sets contained in the history) - this value was 
found to perform best empirically (rather than using a single scalar, or
 the full number of rectangle sets in the history).:</p>
<h3>
<a id="user-content-here-is-the-output-when-each-frame-is-processed-separately" class="anchor" href="#here-is-the-output-when-each-frame-is-processed-separately" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Here is the output when each frame is processed separately:</h3>
<p><a href="file:///Users/atripathy/Vehicle-Detection/test_video1_out.mp4">Test Video Frames processed separately</a></p>
<h3>
<a id="user-content-here-is-the-output-when-we-process-information-using-the-past-frames" class="anchor" href="#here-is-the-output-when-we-process-information-using-the-past-frames" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Here is the output when we process information using the past frames:</h3>
<p><a href="file:///Users/atripathy/Vehicle-Detection/test_video_out_2.mp4">Test Video Frames processed using past rectangles</a></p>
<h3>
<a id="user-content-here-is-the-final-result" class="anchor" href="#here-is-the-final-result" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Here is the final result:</h3>
<p><a href="file:///Users/atripathy/Vehicle-Detection/project_video_out.mp4">Final Project Video</a></p>
<hr>
<h3>
<a id="user-content-discussion" class="anchor" href="#discussion" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Discussion</h3>
<h4>
<a id="user-content-1-briefly-discuss-any-problems--issues-you-faced-in-your-implementation-of-this-project--where-will-your-pipeline-likely-fail--what-could-you-do-to-make-it-more-robust" class="anchor" href="#1-briefly-discuss-any-problems--issues-you-faced-in-your-implementation-of-this-project--where-will-your-pipeline-likely-fail--what-could-you-do-to-make-it-more-robust" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1.
 Briefly discuss any problems / issues you faced in your implementation 
of this project.  Where will your pipeline likely fail?  What could you 
do to make it more robust?</h4>
<p>It was a good experience to learn about the sliding window method for
 object detection. However, I noticed that there are 2 disadvantages:</p>
<ul>
<li>The most important one being that it is computationally very expensive. It requires so many classifier tries per image.</li>
<li>Again for computational reduction not whole area of input image is 
scanned. So when road has another placement in the image like in strong 
curved turns or camera movements, sliding windows may fail to detect 
cars.</li>
</ul>
<p>The <a href="https://arxiv.org/pdf/1512.02325.pdf">Single Shot MultiBox Detector</a> might be a good alternative for the sliding windows approach.</p>
</article></body></html>